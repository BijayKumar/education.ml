# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Shell variables don't work here
# While we could hard-code to the value of this variable,
#   we're specifying --jars on the command line instead.
# This maintains the variable nature of this value.
# This needs to be done for anything that uses spark-submit.
#   ie. spark-sql, spark-shell, spark-submit, etc.
#spark.executor.extraClassPath=$MYSQL_CONNECTOR_JAR
#spark.driver.extraClassPath=$MYSQL_CONNECTOR_JAR
spark.master=spark://127.0.0.1:7077
#spark.executor.cores=10
#spark.executor.memory=10240m
#spark.executor.instances=4
#spark.default.parallelism=4
spark.shuffle.service.enabled=false
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=4
#spark.streaming.backpressure.enabled=true
spark.deploy.defaultCores=1
spark.eventLog.enabled=true
spark.eventLog.dir=/root/pipeline/logs/spark
spark.history.fs.logDirectory=/root/pipeline/logs/spark/spark-events
spark.history.ui.port=5050
spark.sql.parquet.filterPushdown=true
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.mergeSchema=false
spark.hadoop.parquet.enable.summary-metadata=false
spark.sql.orc.filterPushdown=true
spark.sql.orc.splits.include.file.footer=true
spark.sql.orc.cache.stripe.details.size=10000
spark.speculation=false
spark.hadoop.mapreduce.fileoutputcommitter.cleanup.skipped=true
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.fs.s3a.readahead.range=157810688
spark.hadoop.fs.s3a.experimental.input.fadvise=random
spark.hadoop.fs.s3a.fast.output.enable=true
spark.sql.hive.metastorePartitionPruning=true
spark.sql.inMemoryColumnarStorage.partitionPruning=true
#spark.memory.offHeap.enabled=true
#spark.memory.offHeap.size=4096m
#spark.worker.cleanup.enabled=true
#spark.driver.userClassPathFirst=true
#spark.shuffle.compress=true
#spark.shuffe.spill.compress=true
#spark.io.compression.codec=lzf
#spark.io.compression.lzf.blockSize=32k
#spark.hadoop.mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter
#spark.hadoop.mapreduce.use.directfileoutputcommitter=true
#spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
